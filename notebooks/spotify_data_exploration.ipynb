{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e851b6d9-a2a4-47ff-a48b-3891cf9d0567",
   "metadata": {},
   "source": [
    "<h1>Load Dataset and Setup</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803dbe91-acdb-433c-a083-086a30c5ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_num_threads(4)           \n",
    "torch.set_num_interop_threads(4)\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "csv_path = os.path.join('../data', 'spotify_dataset.csv')\n",
    "json1_path = os.path.join('../data', '900k Definitive Spotify Dataset.json')\n",
    "json2_path = os.path.join('../data', 'final_milliondataset_BERT_500K_revised.json')\n",
    "\n",
    "try:\n",
    "    df_csv = pd.read_csv(csv_path, low_memory=False)\n",
    "    print(f\"CSV loaded with {len(df_csv)} rows and {len(df_csv.columns)} columns\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "\n",
    "print(\"CSV dataset info:\")\n",
    "display(df_csv.info())\n",
    "print(\"\\nCSV dataset sample:\")\n",
    "display(df_csv.head())\n",
    "\n",
    "print(\"\\nLoading first JSON dataset sample (first 1000 records)...\")\n",
    "try:\n",
    "    with open(json1_path, 'r', encoding='utf-8') as f:\n",
    "        data_json1 = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 1000:\n",
    "                break\n",
    "            data_json1.append(json.loads(line))\n",
    "    print(f\"Loaded {len(data_json1)} JSON records from first file\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading JSON 1: {e}\")\n",
    "\n",
    "if len(data_json1) > 0:\n",
    "    print(\"Keys in one JSON record:\")\n",
    "    display(data_json1[0].keys())\n",
    "    print(\"\\nSample JSON record:\")\n",
    "    display(data_json1[0])\n",
    "\n",
    "try:\n",
    "    with open(json2_path, 'r', encoding='utf-8') as f:\n",
    "        data_json2 = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 1000:\n",
    "                break\n",
    "            data_json2.append(json.loads(line))\n",
    "    print(f\"Loaded {len(data_json2)} JSON records from first file\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading JSON 1: {e}\")\n",
    "\n",
    "if len(data_json2) > 0:\n",
    "    print(\"Keys in one JSON record:\")\n",
    "    display(data_json2[0].keys())\n",
    "    print(\"\\nSample JSON record:\")\n",
    "    display(data_json2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb31f6bc-84f9-4170-8a7b-6194da615730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For BERT\n",
    "import sys\n",
    "print(sys.executable)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add817e-8b41-4ce0-a143-919deddbe700",
   "metadata": {},
   "source": [
    "<h1>Dataset Cleaning and Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c709fc-d806-44ed-ab7d-1ab79293b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyexpat import model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Convert to dataframes\n",
    "df_json1 = pd.DataFrame(data_json1)\n",
    "df_json2 = pd.DataFrame(data_json2)\n",
    "\n",
    "#Standardize column names\n",
    "def clean_col(df):\n",
    "    df.columns = (df.columns.\n",
    "                  str.strip().\n",
    "                  str.lower().\n",
    "                  str.replace(\" \", \"_\").\n",
    "                  str.replace(\"(\", \"\").\n",
    "                  str.replace(\")\", \"\").\n",
    "                  str.replace(\"/\", \"_\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_csv = clean_col(df_csv)\n",
    "df_json1 = clean_col(df_json1)\n",
    "df_json2 = clean_col(df_json2)\n",
    "\n",
    "#Columns that all 3 share and will be used for regression\n",
    "common_cols = [\n",
    "    \"text\", \"emotion\", \"genre\", \"key\", \"tempo\", \"loudness_db\", \"time_signature\", \"explicit\", \"popularity\", \"energy\", \"danceability\", \n",
    "    \"positiveness\", \"speechiness\", \"liveness\", \"acousticness\", \"instrumentalness\"\n",
    "]\n",
    "\n",
    "#Keep only available columns\n",
    "df_csv = df_csv[[c for c in common_cols if c in df_csv.columns]]\n",
    "df_json1 = df_json1[[c for c in common_cols if c in df_json1.columns]]\n",
    "df_json2 = df_json2[[c for c in common_cols if c in df_json2.columns]]\n",
    "\n",
    "#Merge the three datasets\n",
    "df = pd.concat([df_csv, df_json1, df_json2], ignore_index=True)\n",
    "print(f\"Merged dataset size: {df.shape}\")\n",
    "\n",
    "df = df.drop_duplicates(subset=[\"text\"])\n",
    "df = df[df[\"text\"].notna() & (df[\"text\"].str.len() > 20)]  #Min length\n",
    "df = df[df[\"text\"].str.split().str.len() >= 15]  #More words required\n",
    "\n",
    "#Clean numeric columns and convert types\n",
    "numeric_cols = [\n",
    "    \"tempo\", \"loudness_db\", \"popularity\", \"energy\", \"danceability\", \"positiveness\", \"speechiness\", \"liveness\", \"acousticness\", \"instrumentalness\"\n",
    "]\n",
    "\n",
    "#Keep only the db value, remove unit\n",
    "if \"loudness_db\" in df.columns:\n",
    "    df[\"loudness_db\"] = (\n",
    "        df[\"loudness_db\"]\n",
    "        .astype(str)            \n",
    "        .str.replace(\"db\", \"\")  #remove 'db'\n",
    "        .str.strip()            #remove extra spaces\n",
    "    )\n",
    "    df[\"loudness_db\"] = pd.to_numeric(df[\"loudness_db\"], errors=\"coerce\")\n",
    "\n",
    "#Convert all the columns into numeric\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "#Map explicit to 1 or 0\n",
    "df[\"explicit\"] = df[\"explicit\"].map({\"Yes\": 1, \"No\": 0})\n",
    "df[\"explicit\"] = df[\"explicit\"].fillna(0)\n",
    "\n",
    "#Drop rows missing emotion label - target\n",
    "df = df[df[\"emotion\"].notna()]\n",
    "# Normalize emotion labels\n",
    "df['emotion'] = df['emotion'].str.lower().str.strip()\n",
    "#Keep only joy, sadness, anger\n",
    "keep_emotions = ['joy', 'sadness', 'anger']\n",
    "df = df[df['emotion'].isin(keep_emotions)]\n",
    "print(df['emotion'].value_counts())\n",
    "\n",
    "\n",
    "#Drop rows missing >30% numeric features\n",
    "df.dropna(subset=numeric_cols, thresh=8, inplace=True)\n",
    "\n",
    "#Fill missing values with median\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "        # df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "print(\"After cleaning:\")\n",
    "display(df.head())\n",
    "#Data exploration\n",
    "display(df.describe())\n",
    "print(df[\"emotion\"].value_counts())\n",
    "print(df[\"genre\"].value_counts().head(20))\n",
    "\n",
    "\n",
    "#Emotion Distribution\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.countplot(data=df, x=\"emotion\", order=df[\"emotion\"].value_counts().index)\n",
    "plt.title(\"Emotion Distribution\")\n",
    "plt.show()\n",
    "\n",
    "#Correlation Heatmap\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df[numeric_cols].corr(), annot=True)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    \n",
    "    #Boxplot\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f\"{col} - Boxplot\")\n",
    "    \n",
    "    #Histogram\n",
    "    plt.subplot(1,2,2)\n",
    "    #https://seaborn.pydata.org/generated/seaborn.histplot.html\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f\"{col} - Histogram\")\n",
    "    plt.show()\n",
    "\n",
    "#Find and display outliers\n",
    "def detect_outliers(col):\n",
    "    q1 = df[col].quantile(0.25)\n",
    "    q3 = df[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    return df[(df[col] < lower) | (df[col] > upper)][col]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    outliers = detect_outliers(col)\n",
    "    print(f\"{col}: {len(outliers)} outliers\")\n",
    "    df[col] = winsorize(df[col], limits=[0.01, 0.01])  \n",
    "\n",
    "\n",
    "#Sample 150,000 rows, 50k from each emotion\n",
    "#Using https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html\n",
    "TARGET_SIZE = 150000\n",
    "SAMPLES_PER_EMOTION = 50000\n",
    "\n",
    "df_sampled_list = []\n",
    "for emotion, group in df.groupby(\"emotion\"):\n",
    "    df_sampled_list.append(group.sample(SAMPLES_PER_EMOTION, random_state=42)\n",
    ")\n",
    "\n",
    "df_sampled = pd.concat(df_sampled_list, ignore_index=True)\n",
    "df_sampled = df_sampled.reset_index(drop=True)\n",
    "print(\"Sampled dataset size:\", df_sampled.shape)\n",
    "print(df_sampled[\"emotion\"].value_counts())\n",
    "\n",
    "\n",
    "#Add vader results to dataframe for regression\n",
    "vader_path = os.path.join('../data', 'vader_sentiment_150k.pkl')\n",
    "\n",
    "if os.path.exists(vader_path):\n",
    "    vader_df = pd.read_pickle(vader_path)\n",
    "    df_sampled['original_index'] = df_sampled.index\n",
    "    \n",
    "    df_sampled = df_sampled.merge(vader_df, on='original_index', how='left')\n",
    "    df_sampled = df_sampled.drop('original_index', axis=1)\n",
    "    \n",
    "    print(f\"Dataframe after merging VADER: {df_sampled.shape}\")\n",
    "    display(df_sampled[['vader_neg', 'vader_neu', 'vader_pos', 'vader_compound']].describe())\n",
    "else:\n",
    "    print(\"VADER pickle not found, running analysis\")\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    chunk_size = 5000  #Make higher if using good CPU\n",
    "    output_dir = '../data'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    vader_pickle_path = os.path.join(output_dir, 'vader_sentiment_150k.pkl')\n",
    "    vader_csv_path = os.path.join(output_dir, 'vader_sentiment_150k.csv')\n",
    "\n",
    "    vader_chunks = []\n",
    "\n",
    "    #TQDM to see progress of analysis\n",
    "    for start in tqdm(range(0, len(df_sampled), chunk_size), desc=\"Processing VADER\"):\n",
    "        chunk = df_sampled.iloc[start:start+chunk_size].copy()\n",
    "        chunk_scores = chunk['text'].apply(lambda x: pd.Series(analyzer.polarity_scores(str(x))))\n",
    "        chunk_scores = chunk_scores.rename(columns={\"neg\":\"vader_neg\",\"neu\":\"vader_neu\",\"pos\":\"vader_pos\",\"compound\":\"vader_compound\"})\n",
    "        chunk_scores['original_index'] = range(start, start + len(chunk))\n",
    "        vader_chunks.append(chunk_scores)\n",
    "\n",
    "    vader_df = pd.concat(vader_chunks, ignore_index=True)\n",
    "    vader_df.to_pickle(vader_pickle_path)\n",
    "    vader_df.to_csv(vader_csv_path, index=False)\n",
    "\n",
    "    print(f\"VADER analysis saved to pickle: {vader_pickle_path}\")\n",
    "    print(f\"VADER analysis saved to CSV: {vader_csv_path}\")\n",
    "\n",
    "\n",
    "#Encode categorical\n",
    "#Emotion - label encoding\n",
    "label_enc = LabelEncoder()\n",
    "df_sampled[\"emotion_label\"] = label_enc.fit_transform(df_sampled[\"emotion\"])\n",
    "emotion_mapping = dict(zip(label_enc.transform(label_enc.classes_), label_enc.classes_))\n",
    "print(\"Emotion mapping (label : emotion):\")\n",
    "print(emotion_mapping)\n",
    "\n",
    "#Genre - one hot\n",
    "#Extract only the 1st genre if a song has multiple\n",
    "df_sampled['genre'] = df_sampled['genre'].astype(str).str.split(',').str[0].str.strip()\n",
    "df_sampled['genre'] = df_sampled['genre'].replace(['nan', 'None', ''], 'unknown')\n",
    "print(f\"After extracting first genre: {df_sampled['genre'].nunique()} unique values\")\n",
    "#Take top 10 genres to avoid too many features\n",
    "top_genres = df_sampled['genre'].value_counts().head(10).index\n",
    "df_sampled['genre_grouped'] = df_sampled['genre'].apply(lambda x: x if x in top_genres else 'other')\n",
    "df_sampled = pd.get_dummies(df_sampled, columns=[\"genre_grouped\"], prefix=\"genre\")\n",
    "df_sampled.drop(columns=[\"genre\"], inplace=True)\n",
    "\n",
    "#Key - 24 keys in dataset\n",
    "#One hot encoding\n",
    "df_sampled[\"key\"] = df_sampled[\"key\"].fillna(\"unknown\").astype(str).str.strip()\n",
    "df_sampled[\"key\"] = df_sampled[\"key\"].astype(str).str.strip()\n",
    "df_sampled = pd.get_dummies(df_sampled, columns=[\"key\"], prefix=\"key\")\n",
    "\n",
    "#Time sig\n",
    "#One hot encoding\n",
    "df_sampled[\"time_signature\"] = df_sampled[\"time_signature\"].astype(str)\n",
    "invalid_ts = ['nan', 'None', '', 'unknown']\n",
    "df_sampled = df_sampled[~df_sampled[\"time_signature\"].isin(invalid_ts)]\n",
    "df_sampled = pd.get_dummies(df_sampled, columns=[\"time_signature\"], prefix=\"ts\")\n",
    "\n",
    "print(\"Dataframe after encoding:\\n\")\n",
    "display(df_sampled)\n",
    "\n",
    "#generate BERT \n",
    "#ran into issue with bert pverloading \n",
    "# resource learned from: https://www.kaggle.com/code/harshjain123/bert-for-everyone-tutorial-implementation\n",
    "#df_sampled[\"text\"]=df_sampled[\"text\"].astype(str)\n",
    "#so no recompiling would be needed\n",
    "goPath = os.path.join('../data', 'goemotions.pkl')\n",
    "\n",
    "if os.path.exists(goPath):\n",
    "    #already found the goemo path\n",
    "    emotion_df = pd.read_pickle(goPath)\n",
    "\n",
    "else:\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "    emotionModel=AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "    emotionModel.eval()\n",
    "    ##id we can switch to cuda because it goes faster\n",
    "    print(f\"GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  cuda:{i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    emotionModel.to(device)\n",
    "    def goEmotion(texts, batch=32):\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        result=[]\n",
    "        total_batches = (len(texts) + batch - 1) // batch\n",
    "        for i in range(0,len(texts),batch):\n",
    "            batch_idx = i // batch\n",
    "            batchResult=texts[i:i+batch]\n",
    "            inputs=tokenizer(batchResult,return_tensors=\"pt\", padding=True, truncation=True,max_length=96).to(device)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            ##were having an issue with time constraints and it not loading quickly so using this it doesnt track gradient memory-->making faster\n",
    "            with torch.no_grad():\n",
    "                outputs=emotionModel(**inputs)\n",
    "            logits=outputs.logits\n",
    "            probs=torch.sigmoid(logits).cpu().numpy()\n",
    "            result.append(probs)\n",
    "\n",
    "            if batch_idx % max(1, total_batches//100) == 0 or batch_idx == total_batches - 1:\n",
    "                print(f\"Progress: {batch_idx}/{total_batches} ({batch_idx/total_batches*100:.1f}%)\")\n",
    "                print(f\"  RTX VRAM: {torch.cuda.memory_allocated(0)/1e9:.1f}GB\")\n",
    "        return np.vstack(result)\n",
    "    #gets the emotions\n",
    "    emotionProbability=goEmotion(df_sampled[\"text\"].astype(str).tolist())\n",
    "    #make the labels names\n",
    "    emotionLabels=list(emotionModel.config.id2label.values())\n",
    "    emotion_df = pd.DataFrame(\n",
    "        emotionProbability,\n",
    "        columns=emotionLabels\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(goPath), exist_ok=True)\n",
    "    emotion_df.to_pickle(goPath)\n",
    "\n",
    "    #df_sampled.drop(columns=emotion_df.columns, inplace=True)\n",
    "\n",
    "#make the emotions collide with what we have\n",
    "go_map = {\n",
    "    \"happy\":[\"pride\",\"admiration\",\"amusement\",\"excitement\",\"love\",\"optimism\",\"gratitude\",\"relief\",\"joy\"],\n",
    "    \"sad\":[\"disappointment\",\"remorse\",\"sadness\",\"grief\"],\n",
    "    \"anger\":[\"anger\",\"annoyance\",\"disapproval\",\"disgust\"],\n",
    "    \"fear\":[\"fear\", \"nervousness\"],\n",
    "    \"surprise\":[\"surprise\"],\n",
    "    \"neutral\":[\"neutral\", \"realization\"]\n",
    "}\n",
    "\n",
    "for target_emotion,fine_labels in go_map.items():\n",
    "    valid_labels =[label for label in fine_labels if label in emotion_df.columns]\n",
    "    df_sampled[target_emotion] = emotion_df[valid_labels].sum(axis=1)\n",
    "\n",
    "# merge GoEmotions into sampled dataset\n",
    "df_sampled = pd.concat([df_sampled.reset_index(drop=True), emotion_df.reset_index(drop=True)],axis=1)\n",
    "print(f\"Dataframe after adding BERT embeddings:{df_sampled.shape}\")\n",
    "\n",
    "numeric_cols += ['vader_neg', 'vader_neu', 'vader_pos', 'vader_compound']\n",
    "feature_cols = [c for c in df_sampled.columns \n",
    "                if c not in [\"emotion\", \"emotion_label\", \"text\"]]\n",
    "X = df_sampled[feature_cols]\n",
    "X = X.fillna(0)\n",
    "y = df_sampled[\"emotion_label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train_final = X_train.copy()\n",
    "X_test_final = X_test.copy()\n",
    "\n",
    "# feature engineering\n",
    "numeric_features = numeric_cols + ['vader_neg', 'vader_neu', 'vader_pos', 'vader_compound'] + list(go_map.keys())\n",
    "numeric_features = [f for f in numeric_features if f in X_train_final.columns]\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = X_train_final.copy()\n",
    "X_test_scaled = X_test_final.copy()\n",
    "\n",
    "X_train_scaled[numeric_features] = scaler.fit_transform(X_train_scaled[numeric_features])\n",
    "X_test_scaled[numeric_features] = scaler.transform(X_test_scaled[numeric_features])\n",
    "\n",
    "\n",
    "bert_columns = list(emotion_df.columns) \n",
    "bert_columns = [c for c in bert_columns if c in X_train_scaled.columns]\n",
    "\n",
    "pca_components = min(10, len(bert_columns))\n",
    "pca = PCA(n_components=pca_components, random_state=42)\n",
    "\n",
    "X_train_scaled_pca = X_train_scaled.copy()\n",
    "X_test_scaled_pca = X_test_scaled.copy()\n",
    "\n",
    "X_train_scaled_pca_bert = pca.fit_transform(X_train_scaled_pca[bert_columns])\n",
    "X_test_scaled_pca_bert = pca.transform(X_test_scaled_pca[bert_columns])\n",
    "\n",
    "for i in range(pca_components):\n",
    "    X_train_scaled_pca[f'bert_pca_{i+1}'] = X_train_scaled_pca_bert[:, i]\n",
    "    X_test_scaled_pca[f'bert_pca_{i+1}'] = X_test_scaled_pca_bert[:, i]\n",
    "\n",
    "X_train_scaled_pca.drop(columns=bert_columns, inplace=True)\n",
    "X_test_scaled_pca.drop(columns=bert_columns, inplace=True)\n",
    "\n",
    "k_best = 25\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=k_best)\n",
    "\n",
    "X_train_final_selected = selector.fit_transform(X_train_scaled_pca, y_train)\n",
    "X_test_final_selected = selector.transform(X_test_scaled_pca)\n",
    "\n",
    "selected_feature_names = X_train_scaled_pca.columns[selector.get_support()]\n",
    "\n",
    "X_train_final = pd.DataFrame(X_train_final_selected, columns=selected_feature_names)\n",
    "X_test_final = pd.DataFrame(X_test_final_selected, columns=selected_feature_names)\n",
    "\n",
    "print(\"Final dataset after cleaning, preprocessing, and feature selection:\")\n",
    "print(\"Columns present:\\n\")\n",
    "print(X_train_final.columns)\n",
    "print(\"Number of columns:\\n\")\n",
    "print(X_train_final.shape[1])\n",
    "print(\"X - features:\\n\")\n",
    "display(X_train_final)\n",
    "print(\"Y - target:\\n\")\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187cc029-9bac-4822-bcd4-3277bf0b26a1",
   "metadata": {},
   "source": [
    "<h1>Model Evaluation Function</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50b1e4-7db7-422f-b268-99cfa2cf8484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score\n",
    "import time\n",
    "def train_test_evaluate(model, model_name = \"\", visualize = False, X_train = X_train_final, X_test = X_test_final, y_train = y_train, y_test = y_test):\n",
    "    fit_start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    fit_end = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    pred_end = time.time()\n",
    "    if(visualize):\n",
    "        print(model_name, \"Results\")\n",
    "        print(\"Training time: {:.3f} seconds\".format(fit_end - fit_start))\n",
    "        print(\"Prediction time: {:.3f} seconds\".format(pred_end - fit_end))\n",
    "        print(\"Total time taken: {:.3f} seconds\".format(pred_end - fit_start))\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        cm_rf = confusion_matrix(y_test, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=label_enc.classes_)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(f\"{model_name} Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb265c0-500e-414e-ad90-ea300e226856",
   "metadata": {},
   "source": [
    "<h1>Decision Tree Classifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cb1e7c-c8cd-4152-9e8c-586c2d4bf320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Baseline Decision Tree - Gini\n",
    "dt_base = DecisionTreeClassifier(random_state=42, criterion='gini')\n",
    "y_pred_dt_base = train_test_evaluate(dt_base, visualize = True, model_name = \"Base Decision Tree Classifier (Gini)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30238e9a-6dc5-485d-827b-f03e13dd5d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "# 1. Compact grid over reasonable params\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, None],         \n",
    "    'min_samples_leaf': [1, 5],         \n",
    "    'min_samples_split': [2, 5],        \n",
    "    'ccp_alpha': [0.0, 0.01],           \n",
    "    'criterion': ['gini'],              \n",
    "    'max_features': [None, 'sqrt'],     \n",
    "    'splitter': ['best'],               \n",
    "}\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt_clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_final, y_train)\n",
    "\n",
    "print(\"Best parameters (single GridSearch):\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# 2. Train best tree and evaluate\n",
    "dt_clf_best = DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    **grid_search.best_params_\n",
    ")\n",
    "\n",
    "y_pred_dt_best = train_test_evaluate(dt_clf_best, visualize = True, model_name = \"Best Decision Tree Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c1929-27c8-44c3-8ded-8f1866ff9da6",
   "metadata": {},
   "source": [
    "<h1>Ensemble Methods</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25165e53-00f7-4b34-abc8-8fcec4b9b485",
   "metadata": {},
   "source": [
    "<h3>Random Forest</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f406fe7-a5ef-4114-bc3e-71faece2b2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rf_base = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=100,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "y_pred_rf_base = train_test_evaluate(rf_base, visualize = True, model_name = \"Base Random Forest Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77293766-7875-470f-9a25-8dbc226b3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Much cheaper random search over RF params\n",
    "param_dist = {\n",
    "    'n_estimators': [20, 40],         \n",
    "    'max_depth': [10, None],          \n",
    "    'min_samples_split': [2, 5],      \n",
    "    'min_samples_leaf': [1, 2],       \n",
    "    'bootstrap': [True],              \n",
    "    'max_features': ['sqrt'],         \n",
    "}\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    estimator=rf_clf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=4,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_search.fit(X_train_final, y_train)\n",
    "print(\"Best RF params:\", rf_search.best_params_)\n",
    "\n",
    "# Train and evluate best RF\n",
    "rf_best = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    **rf_search.best_params_\n",
    ")\n",
    "\n",
    "y_pred_rf_grid = train_test_evaluate(rf_best, visualize = True, model_name = \"Best Random Forest Classifier (grid search)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7962d218-1bd3-4cfa-8322-3d9f3c4d7e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Random Forest - Static Parameters\n",
    "\n",
    "# Best parameters from Random Forest Random Search: {'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': 10, 'bootstrap': False}\n",
    "#Best parameters from Random Forest Stage 2 Random Search: {'max_features': 'sqrt', 'criterion': 'gini', 'class_weight': 'balanced'}\n",
    "#Best parameters from Random Forest Stage 3 Random Search: {'min_weight_fraction_leaf': 0.0, 'min_impurity_decrease': 0.0}\n",
    "#Best parameters from Random Forest Stage 4 Random Search: {'warm_start': True, 'oob_score': False}\n",
    "\n",
    "rf_clf_best_random = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=1,\n",
    "    max_depth=10,\n",
    "    bootstrap=False,\n",
    "    max_features='sqrt',\n",
    "    criterion='gini',\n",
    "    class_weight='balanced',\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    min_impurity_decrease=0.0,\n",
    "    warm_start=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "y_pred_rf_random = train_test_evaluate(rf_best, visualize = True, model_name = \"Best Random Forest Classifier (random search)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed7c953-8ab5-4627-aeb4-e0251c8a879e",
   "metadata": {},
   "source": [
    "<h3>AdaBoost</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe9dcd2-4c19-44fe-af8a-c85acc3e54bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#Best parameters from Random Search: {'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 10, 'ccp_alpha': 0.0}\n",
    "#Best parameters from Stage 2 Random Search: {'splitter': 'best', 'max_features': None, 'criterion': 'gini'}\n",
    "#Best parameters from Stage 3 Random Search: {'min_weight_fraction_leaf': 0.01, 'min_impurity_decrease': 0.0, 'max_leaf_nodes': None, 'class_weight': 'balanced'}\n",
    "tuned_dt_clf = DecisionTreeClassifier(\n",
    "        random_state=42,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_depth=10,\n",
    "        ccp_alpha=0.0,\n",
    "        splitter='best',\n",
    "        max_features=None,\n",
    "        criterion='gini',\n",
    "        min_weight_fraction_leaf=0.01,\n",
    "        min_impurity_decrease=0.0,\n",
    "        max_leaf_nodes=None,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "\n",
    "adb = AdaBoostClassifier(\n",
    "    estimator=tuned_dt_clf,\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "y_pred_adb = train_test_evaluate(adb, visualize = True, model_name = \"Base AdaBoost Ensemble Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15be6c49-8d5d-4499-b053-621fc04024af",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# AdaBoost Hyperparameter Tuning\n",
    "param_dist_stage_1 = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1.0],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "adb_clf_random = AdaBoostClassifier(\n",
    "    estimator=adaBoost_dt_clf_best_random,\n",
    "    random_state=42\n",
    ")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0e336-cd44-48ad-94c7-8e479f5b429f",
   "metadata": {},
   "source": [
    "<h3>XGBoost</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4713142-3883-493f-bf05-c737e3e1231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Convert to numeric numpy arrays for XGBoost\n",
    "X_train_xgb = X_train_final.astype('float32').to_numpy()\n",
    "X_test_xgb  = X_test_final.astype('float32').to_numpy()\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='multi:softprob',\n",
    "    num_class=len(label_enc.classes_),\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "y_pred_xgb = train_test_evaluate(xgb, visualize = True, model_name = \"Base XGBoost Ensemble Classifier\", X_train = X_train_xgb, X_test = X_test_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40179a8a-b5ee-4383-ba6c-4010a25c55bc",
   "metadata": {},
   "source": [
    "<h1>Support Vector Machines</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d86d64-73b6-4efa-8f46-601844d42a7f",
   "metadata": {},
   "source": [
    "<h3>SVM with linear kernel</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c69694-73d8-42f1-9400-2ec1a6084068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial run with default hyperparameters\n",
    "from sklearn.svm import LinearSVC\n",
    "linear_svm = LinearSVC(random_state=42)\n",
    "linear_svm_pred = train_test_evaluate(linear_svm, model_name=\"SVM With Linear Kernel\", visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca6fd23-5e6d-47ca-800b-3a86bc3a72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],   \n",
    "    'loss': ['squared_hinge'],  \n",
    "    'dual': [False],               \n",
    "    'tol': [1e-4, 1e-3, 1e-2], \n",
    "    'max_iter': [5000, 10000, 20000], \n",
    "    'fit_intercept': [True, False],\n",
    "}\n",
    "\n",
    "linear_svm = LinearSVC(random_state=42)\n",
    "\n",
    "# random search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=linear_svm,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20, \n",
    "    cv=3, \n",
    "    n_jobs=-1,  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_final, y_train)\n",
    "\n",
    "print(\"Best parameters (single RandomSearch):\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "linear_svm_random_best = LinearSVC(\n",
    "    random_state=42,\n",
    "    **random_search.best_params_\n",
    ")\n",
    "\n",
    "y_pred_linear_svm_random_best = train_test_evaluate(linear_svm_random_best, visualize = True, model_name = \"Best Linear SVM Classifier (random search)\")\n",
    "\n",
    "# Grid search was taking way too long\n",
    "'''# grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=linear_svm,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_final, y_train)\n",
    "\n",
    "print(\"Best parameters (single GridSearch):\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "linear_svm_grid_best = LinearSVC(\n",
    "    random_state=42,\n",
    "    **grid_search.best_params_\n",
    ")\n",
    "\n",
    "y_pred_linear_svm_grid_best = train_test_evaluate(linear_svm_grid_best, visualize = True, model_name = \"Best Linear SVM Classifier (grid search)\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1164042d-6874-45ed-be91-41ae61e5607c",
   "metadata": {},
   "source": [
    "<h3>SVM with RBF kernel</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65df6e5-ef2a-4bec-8924-9dacc0a0153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial run with default hyperparameters\n",
    "from sklearn.svm import SVC\n",
    "# about 1k max iterations is as high as rbf SVM can go without taking too long\n",
    "rbf_svm = SVC(kernel = \"rbf\", max_iter=1000)\n",
    "rbf_svm_pred = train_test_evaluate(rbf_svm, model_name=\"SVM With RBF Kernel\", visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7987ab-7ea8-4e70-bc68-24e2e94474be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],         \n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],                    \n",
    "    'shrinking': [True, False],             \n",
    "    'tol': [1e-3, 1e-2, 1e-1],             \n",
    "}\n",
    "\n",
    "# trying with 1000 max iterations\n",
    "rbf_svm = SVC(kernel = \"rbf\", max_iter=1000, random_state=42)\n",
    "\n",
    "# random search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rbf_svm,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20, \n",
    "    cv=3, \n",
    "    n_jobs=-1,  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_final, y_train)\n",
    "\n",
    "print(\"Best parameters (single RandomSearch):\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "rbf_svm_random_best = SVC(\n",
    "    kernel=\"rbf\",\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    **random_search.best_params_\n",
    ")\n",
    "\n",
    "y_pred_rbf_svm_random_best = train_test_evaluate(rbf_svm_random_best, visualize = True, model_name = \"Best SVM with RBF Kernel Classifier (random search)\")\n",
    "\n",
    "# grid search was taking way too long\n",
    "'''grid_search = GridSearchCV(\n",
    "    estimator=rbf_svm,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_final, y_train)\n",
    "\n",
    "print(\"Best parameters (single GridSearch):\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "rbf_svm_grid_best = LinearSVC(\n",
    "    random_state=42,\n",
    "    **grid_search.best_params_\n",
    ")\n",
    "\n",
    "y_pred_rbf_svm_grid_best = train_test_evaluate(rbf_svm_grid_best, visualize = True, model_name = \"Best SVM with RBF Kernel Classifier (grid search)\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e561a390-09a9-4f71-b270-0527a362aa00",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a02d5-54af-48e2-a819-32c4811cc0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Baseline Logistic Regression\n",
    "\n",
    "for C in [0.25, 0.5, 0.75, 1.0]:\n",
    "    logreg = LogisticRegression(\n",
    "        multi_class='ovr',    \n",
    "        solver='liblinear',\n",
    "        max_iter=500,\n",
    "        C=C,\n",
    "        random_state=42,\n",
    "    )\n",
    "    y_pred_logreg = train_test_evaluate(logreg, model_name=(\"Logistic Regression with C =\", C), visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16df99a-6a7b-45ef-8a50-fa6d6f58d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature augmented\n",
    "logreg_poly = LogisticRegression(\n",
    "    multi_class='ovr',\n",
    "    solver='liblinear',\n",
    "    max_iter=500,\n",
    "    C=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "y_pred_logreg_poly = train_test_evaluate(logreg_poly, model_name=\"Logistic Regression (Feature-augmented)\", visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad79ca4a-a32b-4b56-8319-653855957a3b",
   "metadata": {},
   "source": [
    "<h1>Polynomial Logistic Regression</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514c87e9-25bb-4f03-aa71-4b32de785235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: polynomial logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef67040-c623-453f-b994-2701ce99a43f",
   "metadata": {},
   "source": [
    "<h1>Neural Network with Keras</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac0c394-2c99-46c2-b7a3-e1abc5c98013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "num_features = X_train_final.shape[1]\n",
    "num_classes = len(label_enc.classes_)\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(num_features,)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),  # slightly smaller LR\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_final, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=30,\n",
    "    batch_size=512,\n",
    "    callbacks=[early_stop],\n",
    ")\n",
    "\n",
    "y_proba = model.predict(X_test_final)\n",
    "y_pred_nn = np.argmax(y_proba, axis=1)\n",
    "\n",
    "print(\"Keras NN Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_nn, target_names=label_enc.classes_))\n",
    "\n",
    "cm_nn = confusion_matrix(y_test, y_pred_nn)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_nn, display_labels=label_enc.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Keras Neural Network Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1eb5c6-babe-4741-829b-da922a52eb60",
   "metadata": {},
   "source": [
    "<h1>KNN</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4477e44c-c3f6-48a9-9b2d-ee01ec3b38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# run initial test with default hyperparameters\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "knn_pred = train_test_evaluate(knn, model_name=\"Base KNN Classifier\", visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f653550-5dc5-442b-9d82-841c39f6ed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 31)),    \n",
    "    'weights': ['uniform', 'distance'],      \n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski', 'chebyshev'],  \n",
    "    'p': [1, 2],                             \n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  \n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "# random search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=knn,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20, \n",
    "    cv=3, \n",
    "    n_jobs=-1,  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_final, y_train)\n",
    "\n",
    "print(\"Best parameters (single RandomSearch):\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "knn_best = KNeighborsClassifier(\n",
    "    n_jobs=-1,\n",
    "    **random_search.best_params_\n",
    ")\n",
    "\n",
    "y_pred_knn_random_best = train_test_evaluate(knn_best, visualize = True, model_name = \"Best KNN Classifier (random search)\")\n",
    "\n",
    "# Grid search was taking waaaaaaay too long\n",
    "'''# grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=knn,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_final, y_train_final)\n",
    "\n",
    "print(\"Best parameters (single GridSearch):\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "knn_best = KNeighborsClassifier(\n",
    "    n_jobs=-1,\n",
    "    **grid_search.best_params_\n",
    ")\n",
    "\n",
    "y_pred_knn_grid_best = train_test_evaluate(knn_best, visualize = True, model_name = \"Best KNN Classifier (grid search)\")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
