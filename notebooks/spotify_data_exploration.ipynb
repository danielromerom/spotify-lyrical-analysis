{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e851b6d9-a2a4-47ff-a48b-3891cf9d0567",
   "metadata": {},
   "source": [
    "<h1>Load Dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803dbe91-acdb-433c-a083-086a30c5ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "csv_path = os.path.join('../data', 'spotify_dataset.csv')\n",
    "json1_path = os.path.join('../data', '900k Definitive Spotify Dataset.json')\n",
    "json2_path = os.path.join('../data', 'final_milliondataset_BERT_500K_revised.json')\n",
    "\n",
    "try:\n",
    "    df_csv = pd.read_csv(csv_path, low_memory=False)\n",
    "    print(f\"CSV loaded with {len(df_csv)} rows and {len(df_csv.columns)} columns\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "\n",
    "print(\"CSV dataset info:\")\n",
    "display(df_csv.info())\n",
    "print(\"\\nCSV dataset sample:\")\n",
    "display(df_csv.head())\n",
    "\n",
    "print(\"\\nLoading first JSON dataset sample (first 1000 records)...\")\n",
    "try:\n",
    "    with open(json1_path, 'r', encoding='utf-8') as f:\n",
    "        data_json1 = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 1000:\n",
    "                break\n",
    "            data_json1.append(json.loads(line))\n",
    "    print(f\"Loaded {len(data_json1)} JSON records from first file\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading JSON 1: {e}\")\n",
    "\n",
    "if len(data_json1) > 0:\n",
    "    print(\"Keys in one JSON record:\")\n",
    "    display(data_json1[0].keys())\n",
    "    print(\"\\nSample JSON record:\")\n",
    "    display(data_json1[0])\n",
    "\n",
    "try:\n",
    "    with open(json2_path, 'r', encoding='utf-8') as f:\n",
    "        data_json2 = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 1000:\n",
    "                break\n",
    "            data_json2.append(json.loads(line))\n",
    "    print(f\"Loaded {len(data_json2)} JSON records from first file\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading JSON 1: {e}\")\n",
    "\n",
    "if len(data_json2) > 0:\n",
    "    print(\"Keys in one JSON record:\")\n",
    "    display(data_json2[0].keys())\n",
    "    print(\"\\nSample JSON record:\")\n",
    "    display(data_json2[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add817e-8b41-4ce0-a143-919deddbe700",
   "metadata": {},
   "source": [
    "<h1>Dataset Cleaning and Preprocessing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65619405-9e04-45b1-aa54-a481a84b841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "#Convert to dataframes\n",
    "df_json1 = pd.DataFrame(data_json1)\n",
    "df_json2 = pd.DataFrame(data_json2)\n",
    "\n",
    "#Standardize column names\n",
    "def clean_col(df):\n",
    "    df.columns = (df.columns.\n",
    "                  str.strip().\n",
    "                  str.lower().\n",
    "                  str.replace(\" \", \"_\").\n",
    "                  str.replace(\"(\", \"\").\n",
    "                  str.replace(\")\", \"\").\n",
    "                  str.replace(\"/\", \"_\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_csv = clean_col(df_csv)\n",
    "df_json1 = clean_col(df_json1)\n",
    "df_json2 = clean_col(df_json2)\n",
    "\n",
    "#Columns that all 3 share and will be used for regression\n",
    "common_cols = [\n",
    "    \"text\", \"emotion\", \"genre\", \"key\", \"tempo\", \"loudness_db\", \"time_signature\", \"explicit\", \"popularity\", \"energy\", \"danceability\", \n",
    "    \"positiveness\", \"speechiness\", \"liveness\", \"acousticness\", \"instrumentalness\"\n",
    "]\n",
    "\n",
    "#Keep only available columns\n",
    "df_csv = df_csv[[c for c in common_cols if c in df_csv.columns]]\n",
    "df_json1 = df_json1[[c for c in common_cols if c in df_json1.columns]]\n",
    "df_json2 = df_json2[[c for c in common_cols if c in df_json2.columns]]\n",
    "\n",
    "#Merge the three datasets\n",
    "df = pd.concat([df_csv, df_json1, df_json2], ignore_index=True)\n",
    "print(f\"Merged dataset size: {df.shape}\")\n",
    "\n",
    "\n",
    "#Clean numeric columns and convert types\n",
    "numeric_cols = [\n",
    "    \"tempo\", \"loudness_db\", \"popularity\", \"energy\", \"danceability\", \"positiveness\", \"speechiness\", \"liveness\", \"acousticness\", \"instrumentalness\"\n",
    "]\n",
    "\n",
    "#Keep only the db value, remove unit\n",
    "if \"loudness_db\" in df.columns:\n",
    "    df[\"loudness_db\"] = (\n",
    "        df[\"loudness_db\"]\n",
    "        .astype(str)            \n",
    "        .str.replace(\"db\", \"\")  #remove 'db'\n",
    "        .str.strip()            #remove extra spaces\n",
    "    )\n",
    "    df[\"loudness_db\"] = pd.to_numeric(df[\"loudness_db\"], errors=\"coerce\")\n",
    "\n",
    "#Convert all the columns into numeric\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "#Map explicit to 1 or 0\n",
    "df[\"explicit\"] = df[\"explicit\"].map({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "#Drop rows missing emotion label - target\n",
    "df = df[df[\"emotion\"].notna()]\n",
    "# Normalize emotion labels\n",
    "df['emotion'] = df['emotion'].str.lower().str.strip()\n",
    "#Keep only joy, sadness, anger\n",
    "keep_emotions = ['joy', 'sadness', 'anger']\n",
    "df = df[df['emotion'].isin(keep_emotions)]\n",
    "print(df['emotion'].value_counts())\n",
    "\n",
    "\n",
    "#Drop rows missing >30% numeric features\n",
    "df.dropna(subset=numeric_cols, thresh=7, inplace=True)\n",
    "\n",
    "print(\"After cleaning:\")\n",
    "display(df.head())\n",
    "#Data exploration\n",
    "display(df.describe())\n",
    "print(df[\"emotion\"].value_counts())\n",
    "print(df[\"genre\"].value_counts().head(20))\n",
    "\n",
    "\n",
    "#Emotion Distribution\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.countplot(data=df, x=\"emotion\", order=df[\"emotion\"].value_counts().index)\n",
    "plt.title(\"Emotion Distribution\")\n",
    "plt.show()\n",
    "\n",
    "#Correlation Heatmap\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df[numeric_cols].corr(), annot=True)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(12,8))\n",
    "    \n",
    "    #Boxplot\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f\"{col} - Boxplot\")\n",
    "    \n",
    "    #Histogram\n",
    "    plt.subplot(1,2,2)\n",
    "    #https://seaborn.pydata.org/generated/seaborn.histplot.html\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f\"{col} - Histogram\")\n",
    "    plt.show()\n",
    "\n",
    "#Find and display outliers\n",
    "def detect_outliers(col):\n",
    "    q1 = df[col].quantile(0.25)\n",
    "    q3 = df[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5 * iqr\n",
    "    upper = q3 + 1.5 * iqr\n",
    "    return df[(df[col] < lower) | (df[col] > upper)][col]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    outliers = detect_outliers(col)\n",
    "    print(f\"{col}: {len(outliers)} outliers\")\n",
    "    #TODO - Could cap outliers with winsorization if needed\n",
    "\n",
    "\n",
    "#Sample 150,000 rows, 50k from each emotion\n",
    "#Using https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html\n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html\n",
    "TARGET_SIZE = 150000\n",
    "SAMPLES_PER_EMOTION = 50000\n",
    "\n",
    "df_sampled_list = []\n",
    "for emotion, group in df.groupby(\"emotion\"):\n",
    "    df_sampled_list.append(group.sample(SAMPLES_PER_EMOTION, random_state=37)\n",
    ")\n",
    "\n",
    "df_sampled = pd.concat(df_sampled_list, ignore_index=True)\n",
    "\n",
    "print(\"Sampled dataset size:\", df_sampled.shape)\n",
    "print(df_sampled[\"emotion\"].value_counts())\n",
    "\n",
    "\n",
    "#Add vader results to dataframe for regression\n",
    "vader_path = os.path.join('../data', 'vader_sentiment_150k.pkl')\n",
    "\n",
    "if os.path.exists(vader_path):\n",
    "    vader_df = pd.read_pickle(vader_path)\n",
    "    df_sampled = df_sampled.reset_index(drop=True)\n",
    "    df_sampled['original_index'] = df_sampled.index\n",
    "    \n",
    "    df_sampled = df_sampled.merge(vader_df, on='original_index', how='left')\n",
    "    df_sampled = df_sampled.drop('original_index', axis=1)\n",
    "    \n",
    "    print(f\"Dataframe after merging VADER: {df_sampled.shape}\")\n",
    "    display(df_sampled[['vader_neg', 'vader_neu', 'vader_pos', 'vader_compound']].describe())\n",
    "else:\n",
    "    print(\"VADER pickle not found.\")\n",
    "\n",
    "#Encode categorical\n",
    "#Emotion - label encoding\n",
    "label_enc = LabelEncoder()\n",
    "df_sampled[\"emotion_label\"] = label_enc.fit_transform(df_sampled[\"emotion\"])\n",
    "emotion_mapping = dict(zip(label_enc.transform(label_enc.classes_), label_enc.classes_))\n",
    "print(\"Emotion mapping (label : emotion):\")\n",
    "print(emotion_mapping)\n",
    "\n",
    "#Genre - one hot\n",
    "#Extract only the 1st genre if a song has multiple\n",
    "df_sampled['genre'] = df_sampled['genre'].astype(str).str.split(',').str[0].str.strip()\n",
    "df_sampled['genre'] = df_sampled['genre'].replace(['nan', 'None', ''], 'unknown')\n",
    "print(f\"After extracting first genre: {df_sampled['genre'].nunique()} unique values\")\n",
    "#Take top 20 genres to avoid too many features\n",
    "top_genres = df_sampled['genre'].value_counts().head(20).index\n",
    "df_sampled['genre_grouped'] = df_sampled['genre'].apply(lambda x: x if x in top_genres else 'other')\n",
    "df_sampled = pd.get_dummies(df_sampled, columns=[\"genre_grouped\"], prefix=\"genre\")\n",
    "df_sampled.drop(columns=[\"genre\"], inplace=True)\n",
    "\n",
    "#Key - 24 keys in dataset\n",
    "#One hot encoding\n",
    "df_sampled[\"key\"] = df_sampled[\"key\"].fillna(\"unknown\").astype(str).str.strip()\n",
    "df_sampled[\"key\"] = df_sampled[\"key\"].astype(str).str.strip()\n",
    "df_sampled = pd.get_dummies(df_sampled, columns=[\"key\"], prefix=\"key\")\n",
    "\n",
    "#Time sig\n",
    "#One hot encoding\n",
    "df_sampled[\"time_signature\"] = df_sampled[\"time_signature\"].astype(str)\n",
    "invalid_ts = ['nan', 'None', '', 'unknown']\n",
    "df_sampled = df_sampled[~df_sampled[\"time_signature\"].isin(invalid_ts)]\n",
    "df_sampled = pd.get_dummies(df_sampled, columns=[\"time_signature\"], prefix=\"ts\")\n",
    "\n",
    "print(\"Dataframe after encoding:\\n\")\n",
    "display(df_sampled)\n",
    "\n",
    "\n",
    "numeric_cols += ['vader_neg', 'vader_neu', 'vader_pos', 'vader_compound']\n",
    "feature_cols = [c for c in df_sampled.columns \n",
    "                if c not in [\"emotion\", \"emotion_label\", \"text\"]]\n",
    "X = df_sampled[feature_cols]\n",
    "y = df_sampled[\"emotion_label\"]\n",
    "\n",
    "#TODO - FEATURE SELECTION, IF NEEDED, SHOULD GO HERE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=37, stratify=y\n",
    ")\n",
    "\n",
    "numeric_cols = [\n",
    "    \"tempo\", \"loudness_db\", \"popularity\", \"energy\", \"danceability\", \n",
    "    \"positiveness\", \"speechiness\", \"liveness\", \"acousticness\", \n",
    "    \"instrumentalness\", \"vader_neg\", \"vader_neu\", \"vader_pos\", \n",
    "    \"vader_compound\"\n",
    "]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test distribution:\\n\", y_test.value_counts())\n",
    "\n",
    "print(\"Columns present:\\n\")\n",
    "print(df_sampled.columns)\n",
    "print(\"Number of columns:\\n\")\n",
    "print(df_sampled.shape[1])\n",
    "print(\"X - features:\\n\")\n",
    "display(X)\n",
    "print(\"Y - target:\\n\")\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b7ce8",
   "metadata": {},
   "source": [
    "<h1>Models</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b979f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Baseline Decision Tree - Gini\n",
    "dt_base = DecisionTreeClassifier(random_state=67, criterion='gini')\n",
    "dt_base.fit(X_train, y_train)\n",
    "y_pred_dt_base = dt_base.predict(X_test)\n",
    "\n",
    "cm_base = confusion_matrix(y_test, y_pred_dt_base)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_base, display_labels=label_enc.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues) \n",
    "plt.title(\"Decision Tree Base(Gini) Confusion Matrix\")\n",
    "plt.show() \n",
    "\n",
    "print(\"Decision Tree Classifier - Baseline (Gini)\")\n",
    "print(classification_report(y_test, y_pred_dt_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddf71fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier - Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Grid Search\n",
    "\n",
    "# Stage 1\n",
    "param_grid_stage_1 = {\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'ccp_alpha': [0.0, 0.01, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=67)\n",
    "grid_search_stage_1 = GridSearchCV(estimator=dt_clf, param_grid=param_grid_stage_1, scoring='accuracy', cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search_stage_1.fit(X_train, y_train)\n",
    "print(\"Best parameters from Stage 1 Grid Search:\")\n",
    "print(grid_search_stage_1.best_params_)\n",
    "\n",
    "# Stage 2\n",
    "param_grid_stage_2 = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_features': [None, 'sqrt', 'log2', 5, 10],\n",
    "    'splitter': ['best', 'random'],\n",
    "}\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=67, **grid_search_stage_1.best_params_)\n",
    "grid_search_stage_2 = GridSearchCV(estimator=dt_clf, param_grid=param_grid_stage_2, scoring='accuracy', cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search_stage_2.fit(X_train, y_train)\n",
    "print(\"Best parameters from Stage 2 Grid Search:\")\n",
    "print(grid_search_stage_2.best_params_)\n",
    "\n",
    "# Stage 3\n",
    "param_grid_stage_3 = {\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'min_weight_fraction_leaf': [0.0, 0.01, 0.05, 0.1],\n",
    "    'max_leaf_nodes': [None, 10, 20, 30, 50],\n",
    "    'min_impurity_decrease': [0.0, 0.01, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=67, **grid_search_stage_1.best_params_, **grid_search_stage_2.best_params_)\n",
    "grid_search_stage_3 = GridSearchCV(estimator=dt_clf, param_grid=param_grid_stage_3, scoring='accuracy', cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search_stage_3.fit(X_train, y_train)\n",
    "print(\"Best parameters from Stage 3 Grid Search:\")\n",
    "print(grid_search_stage_3.best_params_)\n",
    "\n",
    "# Best Decision Tree\n",
    "dt_clf_best = DecisionTreeClassifier(\n",
    "    random_state=67,\n",
    "    **grid_search_stage_1.best_params_,\n",
    "    **grid_search_stage_2.best_params_,\n",
    "    **grid_search_stage_3.best_params_\n",
    ")\n",
    "\n",
    "# Show Best Results\n",
    "dt_clf_best.fit(X_train, y_train)\n",
    "y_pred_best = dt_clf_best.predict(X_test)\n",
    "cm_best = confusion_matrix(y_test, y_pred_best)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_best, display_labels=label_enc.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Best Decision Tree (Grid Search) Confusion Matrix\")\n",
    "plt.show()  \n",
    "\n",
    "print(f\"Best Decision Tree Classifier Parameters: {format(dt_clf_best.get_params())}\\n\")\n",
    "print(\"Decision Tree Classifier - Best (Grid Search)\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "# Random Search\n",
    "\n",
    "# Stage 1\n",
    "param_dist_stage_1 = {\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'ccp_alpha': [0.0, 0.01, 0.05, 0.1]\n",
    "}\n",
    "dt_clf_random = DecisionTreeClassifier(random_state=67)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=dt_clf_random,\n",
    "    param_distributions=param_dist_stage_1,\n",
    "    n_iter=50,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=67\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "print(f\"Best parameters from Random Search: {random_search.best_params_}\\n\")\n",
    "\n",
    "# Stage 2\n",
    "param_dist_stage_2 = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_features': [None, 'sqrt', 'log2', 5, 10],\n",
    "    'splitter': ['best', 'random'],\n",
    "}\n",
    "dt_clf = DecisionTreeClassifier(random_state=67, **random_search.best_params_)\n",
    "random_search_stage_2 = RandomizedSearchCV(\n",
    "    estimator=dt_clf,\n",
    "    param_distributions=param_dist_stage_2,\n",
    "    n_iter=30,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=67\n",
    ")\n",
    "random_search_stage_2.fit(X_train, y_train)\n",
    "print(f\"Best parameters from Stage 2 Random Search: {random_search_stage_2.best_params_}\\n\")\n",
    "\n",
    "# Stage 3\n",
    "param_dist_stage_3 = {\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'min_weight_fraction_leaf': [0.0, 0.01, 0.05, 0.1],\n",
    "    'max_leaf_nodes': [None, 10, 20, 30, 50],\n",
    "    'min_impurity_decrease': [0.0, 0.01, 0.05, 0.1]\n",
    "}\n",
    "dt_clf = DecisionTreeClassifier(\n",
    "    random_state=67,\n",
    "    **random_search.best_params_,\n",
    "    **random_search_stage_2.best_params_\n",
    ")\n",
    "random_search_stage_3 = RandomizedSearchCV(\n",
    "    estimator=dt_clf,\n",
    "    param_distributions=param_dist_stage_3,\n",
    "    n_iter=30,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=67\n",
    ")\n",
    "random_search_stage_3.fit(X_train, y_train)\n",
    "print(f\"Best parameters from Stage 3 Random Search: {random_search_stage_3.best_params_}\\n\")\n",
    "\n",
    "# Best Decision Tree from Random Search\n",
    "dt_clf_best_random = DecisionTreeClassifier(\n",
    "    random_state=67,\n",
    "    **random_search.best_params_,\n",
    "    **random_search_stage_2.best_params_,\n",
    "    **random_search_stage_3.best_params_\n",
    ")       \n",
    "\n",
    "# Random Search Best DT\n",
    "dt_clf_best_random.fit(X_train, y_train)\n",
    "y_pred_best_random = dt_clf_best_random.predict(X_test)\n",
    "cm_best_random = confusion_matrix(y_test, y_pred_best_random)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_best_random, display_labels=label_enc.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Best Decision Tree (Random Search) Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best Decision Tree Classifier Parameters (Random Search): {format(dt_clf_best_random.get_params())}\\n\")\n",
    "print(\"Decision Tree Classifier - Best (Random Search)\")\n",
    "print(classification_report(y_test, y_pred_best_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebe8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=67, n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=label_enc.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Random Forest Confusion Matrix (Base)\")\n",
    "plt.show()\n",
    "print(\"Random Forest Classifier\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a551c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Hyperparameter Tuning\n",
    "\n",
    "# Random Search \n",
    "\n",
    "# Stage 1\n",
    "param_dist_stage_1 = {\n",
    "    'n_estimators': [10, 20, 50],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf_clf_random = RandomForestClassifier(random_state=67)\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=rf_clf_random,\n",
    "    param_distributions=param_dist_stage_1,\n",
    "    n_iter=10,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=67\n",
    ")\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "print(f\"Best parameters from Random Forest Random Search: {random_search_rf.best_params_}\\n\")\n",
    "\n",
    "# Stage 2\n",
    "param_dist_stage_2 = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_features': [None, 'sqrt', 'log2', 5, 10],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=67, **random_search_rf.best_params_)\n",
    "random_search_rf_stage_2 = RandomizedSearchCV(\n",
    "    estimator=rf_clf,\n",
    "    param_distributions=param_dist_stage_2,\n",
    "    n_iter=10,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=67\n",
    ")\n",
    "random_search_rf_stage_2.fit(X_train, y_train)\n",
    "print(f\"Best parameters from Random Forest Stage 2 Random Search: {random_search_rf_stage_2.best_params_}\\n\")\n",
    "\n",
    "# Stage 3\n",
    "\n",
    "bootstrap = random_search_rf.best_params_.get('bootstrap', True)\n",
    "\n",
    "bootstrap_true = {\n",
    "    'max_samples': [None, 0.5, 0.75, 1.0],\n",
    "    'min_weight_fraction_leaf': [0.0, 0.01, 0.05, 0.1],\n",
    "    'min_impurity_decrease': [0.0, 0.01, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "bootstrap_false = {\n",
    "    'min_weight_fraction_leaf': [0.0, 0.01, 0.05, 0.1],\n",
    "    'min_impurity_decrease': [0.0, 0.01, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "if bootstrap:\n",
    "    param_dist_stage_3 = bootstrap_true\n",
    "else:\n",
    "    param_dist_stage_3 = bootstrap_false\n",
    "\n",
    "rf_clf = RandomForestClassifier(    \n",
    "    random_state=67,\n",
    "    **random_search_rf.best_params_,\n",
    "    **random_search_rf_stage_2.best_params_\n",
    ")\n",
    "\n",
    "random_search_rf_stage_3 = RandomizedSearchCV(\n",
    "    estimator=rf_clf,\n",
    "    param_distributions=param_dist_stage_3,\n",
    "    n_iter=10,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=67\n",
    ")\n",
    "random_search_rf_stage_3.fit(X_train, y_train)\n",
    "print(f\"Best parameters from Random Forest Stage 3 Random Search: {random_search_rf_stage_3.best_params_}\\n\")\n",
    "\n",
    "param_dist_stage_4 = {\n",
    "    'warm_start': [True, False],\n",
    "    'oob_score': [True, False]\n",
    "}\n",
    "\n",
    "rf_clf = RandomForestClassifier(    \n",
    "    random_state=67,\n",
    "    **random_search_rf.best_params_,\n",
    "    **random_search_rf_stage_2.best_params_,\n",
    "    **random_search_rf_stage_3.best_params_\n",
    ")\n",
    "random_search_rf_stage_4 = RandomizedSearchCV(\n",
    "    estimator=rf_clf,\n",
    "    param_distributions=param_dist_stage_4,\n",
    "    n_iter=20,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=67\n",
    ")\n",
    "random_search_rf_stage_4.fit(X_train, y_train)\n",
    "print(f\"Best parameters from Random Forest Stage 4 Random Search: {random_search_rf_stage_4.best_params_}\\n\")\n",
    "\n",
    "# Best Random Forest - Random Search\n",
    "rf_clf_best_random = RandomForestClassifier(\n",
    "    random_state=67,\n",
    "    **random_search_rf.best_params_,\n",
    "    **random_search_rf_stage_2.best_params_,\n",
    "    **random_search_rf_stage_3.best_params_,\n",
    "    **random_search_rf_stage_4.best_params_\n",
    ")\n",
    "rf_clf_best_random.fit(X_train, y_train)\n",
    "y_pred_best_random = rf_clf_best_random.predict(X_test)\n",
    "cm_best = confusion_matrix(y_test, y_pred_best_random)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_best, display_labels=label_enc.classes_)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Best Random Forest (Random Search) Confusion Matrix\")\n",
    "plt.show()\n",
    "print(f\"Best Random Forest Classifier Parameters (Random Search): {format(rf_clf_best_random.get_params())}\\n\")\n",
    "print(\"Best Random Forest Classifier (Random Search):\")\n",
    "print(classification_report(y_test, y_pred_best_random))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIS4930",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
